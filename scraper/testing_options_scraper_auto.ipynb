{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Options Scraper with PostgreSQL Database - ROBUST VERSION\n",
    "\n",
    "This notebook has been **enhanced** with robust Chrome handling and writes data directly to PostgreSQL:\n",
    "\n",
    "## üöÄ **Robust Chrome Startup Features**\n",
    "1. **Process Cleanup**: Kills existing Chrome processes before starting\n",
    "2. **Incognito Mode**: Avoids user data directory conflicts entirely  \n",
    "3. **Random Debug Ports**: Prevents port conflicts between sessions\n",
    "4. **Retry Logic**: Up to 3 attempts with cleanup between failures\n",
    "5. **Comprehensive Chrome Flags**: Optimized for server/VM environments\n",
    "\n",
    "## üîß **Enhanced Login Handling**\n",
    "1. **Multiple Click Methods**: Normal, ActionChains, JavaScript, form submit\n",
    "2. **Element Visibility**: Scrolls to elements before interaction\n",
    "3. **Robust Error Handling**: Graceful fallbacks for click interception\n",
    "4. **Retry Mechanism**: Multiple login attempts with detailed feedback\n",
    "\n",
    "## üíæ **PostgreSQL Integration**\n",
    "1. **Direct Database Writes**: No intermediate SQLite database\n",
    "2. **Secure Credentials**: Uses `config/credentials.json` for all authentication\n",
    "3. **Real-time Availability**: Data appears instantly in utility notebooks\n",
    "4. **PostgreSQL Syntax**: Proper `%s` parameter placeholders\n",
    "\n",
    "## üõ†Ô∏è **Setup Requirements**\n",
    "\n",
    "1. **Update web scraping credentials** in `config/credentials.json`:\n",
    "   ```json\n",
    "   {\n",
    "     \"web_scraping\": {\n",
    "       \"optionrecom\": {\n",
    "         \"username\": \"your_actual_username_or_email\",\n",
    "         \"password\": \"your_actual_password\"\n",
    "       }\n",
    "     }\n",
    "   }\n",
    "   ```\n",
    "\n",
    "2. **Ensure PostgreSQL access** is working (credentials already configured)\n",
    "\n",
    "3. **Run the scraper** - it will handle all Chrome conflicts automatically\n",
    "\n",
    "## üéØ **Expected Behavior**\n",
    "\n",
    "```\n",
    "üßπ Cleaning up existing Chrome processes...\n",
    "üöÄ Attempting to start Chrome (attempt 1/3)...\n",
    "‚úÖ Chrome started successfully!\n",
    "üîê Using credentials from config/credentials.json\n",
    "üîë Starting automated login process...\n",
    "‚úÖ Login successful! Proceeding with data scraping...\n",
    "üéØ Total records saved to PostgreSQL database: 7\n",
    "üìä Records by strategy and tab (last hour) in PostgreSQL:\n",
    "  Bear Call - Mild Risk 95-97% accuracy > shorter expiry: 2 records\n",
    "  Bull Put - Minimal Risk 97-99% accuracy > longer expiry: 2 records\n",
    "üìà Total records in PostgreSQL database: 518\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import psycopg2\n",
    "import tempfile\n",
    "import uuid\n",
    "import shutil\n",
    "import random\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementClickInterceptedException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import re\n",
    "import hashlib\n",
    "import coolname\n",
    "from datetime import datetime\n",
    "\n",
    "# Add paths for database configuration\n",
    "sys.path.append('../config')\n",
    "sys.path.append('../database')\n",
    "\n",
    "# Load PostgreSQL credentials\n",
    "with open('../config/credentials.json', 'r') as f:\n",
    "    creds = json.load(f)\n",
    "\n",
    "pg_creds = creds['database']['postgresql']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_credentials_from_json():\n",
    "    \"\"\"\n",
    "    Load username and password from credentials.json file\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (username, password) or (None, None) if credentials not found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        credentials_file = '../config/credentials.json'\n",
    "        \n",
    "        if not os.path.exists(credentials_file):\n",
    "            print(f\"Credentials file '{credentials_file}' not found.\")\n",
    "            print(\"Please ensure config/credentials.json exists with web_scraping section.\")\n",
    "            return None, None\n",
    "        \n",
    "        with open(credentials_file, 'r') as f:\n",
    "            creds = json.load(f)\n",
    "        \n",
    "        # Extract web scraping credentials\n",
    "        web_creds = creds.get('web_scraping', {}).get('optionrecom', {})\n",
    "        username = web_creds.get('username')\n",
    "        password = web_creds.get('password')\n",
    "        \n",
    "        if not username or not password:\n",
    "            print(\"Web scraping credentials not found in credentials.json\")\n",
    "            print(\"Please add the following to config/credentials.json:\")\n",
    "            print('\"web_scraping\": {')\n",
    "            print('  \"optionrecom\": {')\n",
    "            print('    \"username\": \"your_username_or_email\",')\n",
    "            print('    \"password\": \"your_password\"')\n",
    "            print('  }')\n",
    "            print('}')\n",
    "            return None, None\n",
    "        \n",
    "        if username == \"your_username_or_email\" or password == \"your_password\":\n",
    "            print(\"Please update the web scraping credentials in config/credentials.json\")\n",
    "            print(\"Current values are placeholder text that need to be replaced.\")\n",
    "            return None, None\n",
    "        \n",
    "        print(\"‚úÖ Web scraping credentials loaded from config/credentials.json\")\n",
    "        return username, password\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading credentials from JSON file: {str(e)}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automated_login(driver, username, password, max_retries=3):\n",
    "    \"\"\"\n",
    "    Perform automated login to optionrecom.com with improved click handling\n",
    "    \n",
    "    Parameters:\n",
    "    driver: Selenium WebDriver instance\n",
    "    username (str): Username or email\n",
    "    password (str): Password\n",
    "    max_retries (int): Maximum number of login attempts\n",
    "    \n",
    "    Returns:\n",
    "    bool: True if login successful, False otherwise\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Login attempt {attempt + 1} of {max_retries}...\")\n",
    "            \n",
    "            # Navigate to login page\n",
    "            driver.get(\"https://optionrecom.com/my-account-2/\")\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Wait for and find username field\n",
    "            username_field = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.NAME, \"username\"))\n",
    "            )\n",
    "            \n",
    "            # Clear and enter username\n",
    "            username_field.clear()\n",
    "            username_field.send_keys(username)\n",
    "            print(\"Username entered successfully\")\n",
    "            \n",
    "            # Find and enter password\n",
    "            password_field = driver.find_element(By.NAME, \"password\")\n",
    "            password_field.clear()\n",
    "            password_field.send_keys(password)\n",
    "            print(\"Password entered successfully\")\n",
    "            \n",
    "            # Find and click login button with improved handling\n",
    "            login_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[@name='login']\"))\n",
    "            )\n",
    "            \n",
    "            # Scroll to button and ensure it's visible\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', behavior: 'smooth'});\", login_button)\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # Try multiple click methods\n",
    "            click_successful = False\n",
    "            \n",
    "            # Method 1: Wait for element to be clickable and try normal click\n",
    "            try:\n",
    "                WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, \"//button[@name='login']\")))\n",
    "                login_button.click()\n",
    "                click_successful = True\n",
    "                print(\"Login button clicked (normal click)\")\n",
    "            except ElementClickInterceptedException:\n",
    "                print(\"Normal click intercepted, trying alternative methods...\")\n",
    "            \n",
    "            # Method 2: ActionChains click\n",
    "            if not click_successful:\n",
    "                try:\n",
    "                    actions = ActionChains(driver)\n",
    "                    actions.move_to_element(login_button).click().perform()\n",
    "                    click_successful = True\n",
    "                    print(\"Login button clicked (ActionChains)\")\n",
    "                except Exception as e:\n",
    "                    print(f\"ActionChains click failed: {str(e)}\")\n",
    "            \n",
    "            # Method 3: JavaScript click\n",
    "            if not click_successful:\n",
    "                try:\n",
    "                    driver.execute_script(\"arguments[0].click();\", login_button)\n",
    "                    click_successful = True\n",
    "                    print(\"Login button clicked (JavaScript)\")\n",
    "                except Exception as e:\n",
    "                    print(f\"JavaScript click failed: {str(e)}\")\n",
    "            \n",
    "            # Method 4: Submit the form instead\n",
    "            if not click_successful:\n",
    "                try:\n",
    "                    form = driver.find_element(By.XPATH, \"//form[.//button[@name='login']]\")\n",
    "                    driver.execute_script(\"arguments[0].submit();\", form)\n",
    "                    click_successful = True\n",
    "                    print(\"Login form submitted\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Form submit failed: {str(e)}\")\n",
    "            \n",
    "            if not click_successful:\n",
    "                raise Exception(\"All click methods failed\")\n",
    "            \n",
    "            # Wait for page to load and check if login was successful\n",
    "            time.sleep(5)\n",
    "            \n",
    "            # Check if we're still on the login page (indicates failed login)\n",
    "            current_url = driver.current_url\n",
    "            if \"my-account\" in current_url and \"login\" not in current_url.lower():\n",
    "                print(\"Login successful!\")\n",
    "                return True\n",
    "            \n",
    "            # Check for error messages\n",
    "            error_elements = driver.find_elements(By.CSS_SELECTOR, \".woocommerce-error, .error, [class*='error']\")\n",
    "            if error_elements:\n",
    "                error_text = error_elements[0].text\n",
    "                print(f\"Login failed: {error_text}\")\n",
    "            else:\n",
    "                print(\"Login may have failed - still on login page\")\n",
    "                \n",
    "        except TimeoutException:\n",
    "            print(f\"Timeout on attempt {attempt + 1} - page took too long to load\")\n",
    "        except NoSuchElementException as e:\n",
    "            print(f\"Could not find login element on attempt {attempt + 1}: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error on attempt {attempt + 1}: {str(e)}\")\n",
    "        \n",
    "        if attempt < max_retries - 1:\n",
    "            print(f\"Retrying in 3 seconds...\")\n",
    "            time.sleep(3)\n",
    "    \n",
    "    print(f\"Login failed after {max_retries} attempts\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trade_id(scrape_date, strategy_type, tab_name, ticker, trigger_price, strike_price):\n",
    "    \"\"\"\n",
    "    Generate a human-readable 3-word trade_id using coolname library.\n",
    "    \n",
    "    Args:\n",
    "        scrape_date: Date when data was scraped\n",
    "        strategy_type: Type of strategy (Bear Call, Bull Put, etc.)\n",
    "        tab_name: Risk level and expiry category\n",
    "        ticker: Stock ticker symbol\n",
    "        trigger_price: Price that triggers the strategy\n",
    "        strike_price: Strike prices for the option spread\n",
    "    \n",
    "    Returns:\n",
    "        str: Human-readable trade ID like 'certain-magpie-dancing'\n",
    "    \"\"\"\n",
    "    # Convert all inputs to strings and handle None values\n",
    "    components = [\n",
    "        str(scrape_date) if scrape_date is not None else '',\n",
    "        str(strategy_type) if strategy_type is not None else '',\n",
    "        str(tab_name) if tab_name is not None else '',\n",
    "        str(ticker) if ticker is not None else '',\n",
    "        str(trigger_price) if trigger_price is not None else '',\n",
    "        str(strike_price) if strike_price is not None else ''\n",
    "    ]\n",
    "    \n",
    "    # Join components with delimiter\n",
    "    combined_string = '|'.join(components)\n",
    "    \n",
    "    # Generate hash and use as seed for reproducible results\n",
    "    hash_value = hashlib.sha256(combined_string.encode('utf-8')).hexdigest()\n",
    "    hash_seed = int(hash_value[:8], 16)\n",
    "    \n",
    "    # Set random seed for deterministic results\n",
    "    random.seed(hash_seed)\n",
    "    \n",
    "    # Generate 3-word coolname\n",
    "    trade_id = '-'.join(coolname.generate(3))\n",
    "    \n",
    "    return trade_id\n",
    "\n",
    "def connect_to_database():\n",
    "    \"\"\"Connect to the PostgreSQL database\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host=pg_creds['host'],\n",
    "            port=pg_creds['port'],\n",
    "            database=pg_creds['database'],\n",
    "            user=pg_creds['user'],\n",
    "            password=pg_creds['password']\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Verify the database has the required table\n",
    "        cursor.execute(\"SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'option_strategies')\")\n",
    "        if not cursor.fetchone()[0]:\n",
    "            print(f\"Error: PostgreSQL database does not contain the option_strategies table.\")\n",
    "            print(\"Please run the database setup script first.\")\n",
    "            conn.close()\n",
    "            return None, None\n",
    "            \n",
    "        print(f\"‚úÖ Connected to PostgreSQL: {pg_creds['host']}\")\n",
    "        return conn, cursor\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to PostgreSQL database: {str(e)}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date(driver):\n",
    "    \"\"\"Extract the date from the page\"\"\"\n",
    "    try:\n",
    "        # Search the page text\n",
    "        page_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "        date_pattern = r'(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d+\\w*,\\s+\\d{4}'\n",
    "        matches = re.findall(date_pattern, page_text)\n",
    "        \n",
    "        if matches:\n",
    "            return matches[0]\n",
    "        \n",
    "        return \"Date not found\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting date: {str(e)}\")\n",
    "        return \"Date extraction error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_options_expiry_date(driver, tab_content=None):\n",
    "    \"\"\"Extract the Options Expiry Date\"\"\"\n",
    "    try:\n",
    "        # Search entire page\n",
    "        page_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "        date_match = re.search(r'Options Expiry Date:?\\s*(\\d{4}-\\d{2}-\\d{2})', page_text)\n",
    "        if date_match:\n",
    "            return date_match.group(1)\n",
    "        \n",
    "        # More general search\n",
    "        date_match = re.search(r'(\\d{4}-\\d{2}-\\d{2})', page_text)\n",
    "        if date_match:\n",
    "            return date_match.group(1)\n",
    "        \n",
    "        return \"Expiry date not found\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting options expiry date: {str(e)}\")\n",
    "        return \"Expiry date extraction error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_active_trades_table(table):\n",
    "    \"\"\"Check if a table contains active trades (not closed trade perspectives)\"\"\"\n",
    "    try:\n",
    "        # Get table text to check for closed trade indicators\n",
    "        table_text = table.text.lower()\n",
    "        \n",
    "        # Look for indicators of closed trades\n",
    "        closed_indicators = [\n",
    "            'closed trade perspectives',\n",
    "            'historical',\n",
    "            'past performance',\n",
    "            'expired',\n",
    "            'completed'\n",
    "        ]\n",
    "        \n",
    "        # Check if any closed indicators are present\n",
    "        for indicator in closed_indicators:\n",
    "            if indicator in table_text:\n",
    "                return False\n",
    "        \n",
    "        # Check the parent container for closed trade indicators\n",
    "        parent_elements = []\n",
    "        current = table\n",
    "        for _ in range(3):  # Check up to 3 levels up\n",
    "            try:\n",
    "                current = current.find_element(By.XPATH, '..')\n",
    "                parent_elements.append(current)\n",
    "            except:\n",
    "                break\n",
    "        \n",
    "        for parent in parent_elements:\n",
    "            try:\n",
    "                parent_text = parent.text.lower()\n",
    "                for indicator in closed_indicators:\n",
    "                    if indicator in parent_text:\n",
    "                        return False\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking if table is active trades: {str(e)}\")\n",
    "        return True  # Default to True if we can't determine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_table_in_tab(driver, tab_content):\n",
    "    \"\"\"Find the best table within a tab, prioritizing active trades\"\"\"\n",
    "    tables = []\n",
    "    \n",
    "    # First try to find tables within the tab content\n",
    "    if tab_content:\n",
    "        try:\n",
    "            tables_in_tab = tab_content.find_elements(By.TAG_NAME, \"table\")\n",
    "            tables.extend(tables_in_tab)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # If no tables in tab content, look for visible tables on the page\n",
    "    if not tables:\n",
    "        all_tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "        visible_tables = [t for t in all_tables if t.is_displayed()]\n",
    "        tables = visible_tables\n",
    "    \n",
    "    if not tables:\n",
    "        return None\n",
    "    \n",
    "    # Filter for active trades tables and tables with data\n",
    "    best_table = None\n",
    "    best_score = -1\n",
    "    \n",
    "    for table in tables:\n",
    "        try:\n",
    "            # Check if table has data rows\n",
    "            rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "            data_rows = [r for r in rows[1:] if r.find_elements(By.TAG_NAME, \"td\")]  # Skip header\n",
    "            \n",
    "            if not data_rows:\n",
    "                continue\n",
    "            \n",
    "            score = len(data_rows)  # Base score on number of data rows\n",
    "            \n",
    "            # Prioritize active trades tables\n",
    "            if is_active_trades_table(table):\n",
    "                score += 1000  # Big bonus for active trades\n",
    "            \n",
    "            # Check if table has expected columns\n",
    "            headers = table.find_elements(By.TAG_NAME, \"th\")\n",
    "            header_texts = [h.text.upper() for h in headers]\n",
    "            \n",
    "            expected_columns = ['TICKER', 'SYMBOL', 'TRIGGER', 'STRIKE', 'PREMIUM']\n",
    "            column_matches = sum(1 for col in expected_columns if any(col in h for h in header_texts))\n",
    "            score += column_matches * 10\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_table = table\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating table: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return best_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table_data(driver, tab, tab_index, date_info, strategy_type, conn, cursor):\n",
    "    \"\"\"Extract data from the table in the current tab and save to PostgreSQL database - WITH TRADE_ID\"\"\"\n",
    "    try:\n",
    "        tab_name = tab.text.strip().replace('\\n', ' ')\n",
    "        print(f\"\\nProcessing Tab {tab_index+1}: '{tab_name}'\")\n",
    "        \n",
    "        # Click the tab\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", tab)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        try:\n",
    "            tab.click()\n",
    "        except:\n",
    "            driver.execute_script(\"arguments[0].click();\", tab)\n",
    "        \n",
    "        time.sleep(3)  # Give more time for content to load\n",
    "        \n",
    "        # Find tab content\n",
    "        tab_href = tab.get_attribute(\"href\")\n",
    "        tab_content = None\n",
    "        \n",
    "        if tab_href and \"#\" in tab_href:\n",
    "            tab_id = tab_href.split(\"#\")[1]\n",
    "            try:\n",
    "                tab_content = WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.ID, tab_id))\n",
    "                )\n",
    "                print(f\"Found tab content for ID: {tab_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not find tab content for ID {tab_id}: {str(e)}\")\n",
    "        \n",
    "        # Extract options expiry date\n",
    "        options_expiry_date = extract_options_expiry_date(driver, tab_content)\n",
    "        \n",
    "        # Find the best table for this tab\n",
    "        table = find_best_table_in_tab(driver, tab_content)\n",
    "        \n",
    "        if not table:\n",
    "            print(f\"No suitable tables found in tab #{tab_index+1}\")\n",
    "            return 0\n",
    "        \n",
    "        # Check if this is an active trades table\n",
    "        is_active = is_active_trades_table(table)\n",
    "        print(f\"Table type: {'Active trades' if is_active else 'Closed/Historical trades'}\")\n",
    "        \n",
    "        if not is_active:\n",
    "            print(f\"Skipping closed trade perspectives table in tab #{tab_index+1}\")\n",
    "            return 0\n",
    "        \n",
    "        # Extract headers\n",
    "        headers = table.find_elements(By.TAG_NAME, \"th\")\n",
    "        header_texts = [header.text.strip() for header in headers]\n",
    "        print(f\"Table headers: {header_texts}\")\n",
    "        \n",
    "        # Find column indices with improved matching\n",
    "        column_map = {\n",
    "            'ID': -1,\n",
    "            'Ticker': -1,\n",
    "            'Trigger Price': -1,\n",
    "            'Strike Price': -1,\n",
    "            'Estimated Premium': -1\n",
    "        }\n",
    "        \n",
    "        for i, header in enumerate(header_texts):\n",
    "            h_upper = header.upper()\n",
    "            if 'ID' in h_upper and column_map['ID'] == -1:\n",
    "                column_map['ID'] = i\n",
    "            elif ('TICKER' in h_upper or 'SYMBOL' in h_upper) and column_map['Ticker'] == -1:\n",
    "                column_map['Ticker'] = i\n",
    "            elif ('TRIGGER' in h_upper and 'PRICE' in h_upper) or 'ENTRY' in h_upper and column_map['Trigger Price'] == -1:\n",
    "                column_map['Trigger Price'] = i\n",
    "            elif 'STRIKE' in h_upper and 'PRICE' in h_upper and column_map['Strike Price'] == -1:\n",
    "                column_map['Strike Price'] = i\n",
    "            elif ('PREMIUM' in h_upper or 'ESTIMATED' in h_upper) and column_map['Estimated Premium'] == -1:\n",
    "                column_map['Estimated Premium'] = i\n",
    "        \n",
    "        print(f\"Column mapping: {column_map}\")\n",
    "        \n",
    "        # Extract rows\n",
    "        rows = table.find_elements(By.TAG_NAME, \"tr\")[1:]  # Skip header\n",
    "        records_count = 0\n",
    "        \n",
    "        print(f\"Found {len(rows)} data rows\")\n",
    "        \n",
    "        for row_idx, row in enumerate(rows):\n",
    "            try:\n",
    "                cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                if not cells:\n",
    "                    continue\n",
    "                \n",
    "                print(f\"Row {row_idx + 1}: {len(cells)} cells\")\n",
    "                \n",
    "                # Extract data from cells with bounds checking\n",
    "                item_id = cells[column_map['ID']].text.strip() if column_map['ID'] != -1 and column_map['ID'] < len(cells) else f\"AUTO_{row_idx+1}\"\n",
    "                ticker_raw = cells[column_map['Ticker']].text.strip() if column_map['Ticker'] != -1 and column_map['Ticker'] < len(cells) else 'N/A'\n",
    "\n",
    "                # Check if ticker contains (ER) and process accordingly\n",
    "                er_value = 0\n",
    "                if \"(ER)\" in ticker_raw:\n",
    "                    ticker = ticker_raw.replace(\"(ER)\", \"\").strip()\n",
    "                    er_value = 1\n",
    "                else:\n",
    "                    ticker = ticker_raw\n",
    "\n",
    "                # Skip records with invalid ticker values\n",
    "                if not ticker or ticker.lower() in ['n/a', 'none', '', 'null']:\n",
    "                    print(f\"Skipping row {row_idx + 1} - invalid ticker: '{ticker}'\")\n",
    "                    continue\n",
    "\n",
    "                trigger_price = cells[column_map['Trigger Price']].text.strip() if column_map['Trigger Price'] != -1 and column_map['Trigger Price'] < len(cells) else 'N/A'\n",
    "                strike_price = cells[column_map['Strike Price']].text.strip() if column_map['Strike Price'] != -1 and column_map['Strike Price'] < len(cells) else 'N/A'\n",
    "                estimated_premium = cells[column_map['Estimated Premium']].text.strip() if column_map['Estimated Premium'] != -1 and column_map['Estimated Premium'] < len(cells) else 'N/A'\n",
    "\n",
    "                # Parse 'strike_price' to extract 'buy' and 'sell' values\n",
    "                strike_buy_value, strike_sell_value = 0.0, 0.0\n",
    "                if \" - \" in strike_price:\n",
    "                    parts = strike_price.split(\" - \")\n",
    "                    if len(parts) == 2:\n",
    "                        try:\n",
    "                            strike_sell_part = parts[0].strip()\n",
    "                            strike_buy_part = parts[1].strip()\n",
    "\n",
    "                            # Extract numerical values more robustly\n",
    "                            sell_match = re.search(r'(\\d+\\.?\\d*)', strike_sell_part)\n",
    "                            buy_match = re.search(r'(\\d+\\.?\\d*)', strike_buy_part)\n",
    "                            \n",
    "                            if sell_match:\n",
    "                                strike_sell_value = float(sell_match.group(1))\n",
    "                            if buy_match:\n",
    "                                strike_buy_value = float(buy_match.group(1))\n",
    "                        except ValueError as e:\n",
    "                            print(f\"Error parsing strike prices: {str(e)}\")\n",
    "\n",
    "                # Generate trade_id for this record\n",
    "                scrape_date_iso = datetime.now().isoformat()\n",
    "                trade_id = generate_trade_id(\n",
    "                    scrape_date_iso, strategy_type, tab_name, \n",
    "                    ticker, trigger_price, strike_price\n",
    "                )\n",
    "\n",
    "                print(f\"Processing: {ticker} | Trigger: {trigger_price} | Strike: {strike_price} | Premium: {estimated_premium} | Trade ID: {trade_id[:20]}...\")\n",
    "                \n",
    "                # PostgreSQL database insert with trade_id - UPDATED\n",
    "                cursor.execute('''\n",
    "                INSERT INTO option_strategies (\n",
    "                    scrape_date, strategy_type, tab_name, ticker, trigger_price, \n",
    "                    strike_price, strike_buy, strike_sell, estimated_premium, item_id, \n",
    "                    options_expiry_date, date_info, er, trade_id\n",
    "                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                ''', (\n",
    "                    scrape_date_iso, strategy_type, tab_name, ticker, trigger_price, \n",
    "                    strike_price, strike_buy_value, strike_sell_value, estimated_premium, item_id, \n",
    "                    options_expiry_date, date_info, er_value, trade_id\n",
    "                ))\n",
    "\n",
    "                conn.commit()\n",
    "                records_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {row_idx + 1}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Successfully saved {records_count} records from tab #{tab_index+1}\")\n",
    "        return records_count\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing tab #{tab_index+1}: {str(e)}\")\n",
    "        import traceback\n",
    "        print(f\"Full traceback: {traceback.format_exc()}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_strategy_page(driver, strategy_url, strategy_type, conn, cursor):\n",
    "    \"\"\"Process a single strategy page and extract data from all tabs\"\"\"\n",
    "    try:\n",
    "        print(f\"\\n===== Processing {strategy_type} Strategy Page =====\")\n",
    "        driver.get(strategy_url)\n",
    "        time.sleep(5)  # Give more time for page to load\n",
    "        \n",
    "        # Extract the date\n",
    "        date_info = extract_date(driver)\n",
    "        print(f\"Page date: {date_info}\")\n",
    "        \n",
    "        # Find all tabs using multiple methods\n",
    "        tabs = []\n",
    "        tab_selectors = [\n",
    "            \"//div[contains(@class, 'ep_tabs_header')]//a[contains(@class, 'ep_label_main')]\",\n",
    "            \"//a[contains(@class, 'ep_label_main')]\",\n",
    "            \"//div[contains(@class, 'tabs')]//a\",\n",
    "            \"//ul[contains(@class, 'tabs')]//a\",\n",
    "            \"//div[contains(@class, 'tab')]//a\"\n",
    "        ]\n",
    "        \n",
    "        for selector in tab_selectors:\n",
    "            try:\n",
    "                found_tabs = driver.find_elements(By.XPATH, selector)\n",
    "                if found_tabs:\n",
    "                    tabs = found_tabs\n",
    "                    print(f\"Found {len(tabs)} tabs using selector: {selector}\")\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Selector {selector} failed: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not tabs:\n",
    "            print(f\"No tab elements found on the {strategy_type} page\")\n",
    "            # Try to process any tables found on the page\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            if tables:\n",
    "                print(f\"Found {len(tables)} tables without tabs, attempting to process...\")\n",
    "                # Create a dummy tab for processing\n",
    "                class DummyTab:\n",
    "                    def __init__(self, index):\n",
    "                        self.index = index\n",
    "                    def text(self):\n",
    "                        return f\"Table {self.index + 1}\"\n",
    "                    def get_attribute(self, attr):\n",
    "                        return None\n",
    "                    def click(self):\n",
    "                        pass\n",
    "                \n",
    "                dummy_tab = DummyTab(0)\n",
    "                return extract_table_data(driver, dummy_tab, 0, date_info, strategy_type, conn, cursor)\n",
    "            return 0\n",
    "        \n",
    "        # Print tab information\n",
    "        for i, tab in enumerate(tabs):\n",
    "            try:\n",
    "                tab_text = tab.text.strip().replace('\\n', ' ')\n",
    "                tab_href = tab.get_attribute('href')\n",
    "                print(f\"Tab {i+1}: '{tab_text}' -> {tab_href}\")\n",
    "            except:\n",
    "                print(f\"Tab {i+1}: Unable to get text/href\")\n",
    "        \n",
    "        # Process only the first 4 tabs\n",
    "        num_tabs_to_process = min(4, len(tabs))\n",
    "        total_records = 0\n",
    "        \n",
    "        for i, tab in enumerate(tabs[:num_tabs_to_process]):\n",
    "            records = extract_table_data(driver, tab, i, date_info, strategy_type, conn, cursor)\n",
    "            total_records += records\n",
    "            time.sleep(2)  # Small delay between tabs\n",
    "        \n",
    "        return total_records\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {strategy_type} strategy page: {str(e)}\")\n",
    "        import traceback\n",
    "        print(f\"Full traceback: {traceback.format_exc()}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_option_strategies_automated(browser_type=\"chrome\", \n",
    "                                     keep_browser_open=True):\n",
    "    \"\"\"\n",
    "    HEADLESS VERSION: Uses headless mode to completely avoid user data directory conflicts\n",
    "    This approach should work even in restricted environments\n",
    "    \n",
    "    Parameters:\n",
    "    browser_type (str): 'chrome' or 'edge'\n",
    "    keep_browser_open (bool): Keep browser open after scraping to maintain session\n",
    "    \n",
    "    Returns:\n",
    "    int: Number of records added to the database\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load credentials from JSON file\n",
    "    username, password = load_credentials_from_json()\n",
    "    if not username or not password:\n",
    "        return 0\n",
    "    \n",
    "    # Connect to PostgreSQL database\n",
    "    conn, cursor = connect_to_database()\n",
    "    if not conn or not cursor:\n",
    "        return 0\n",
    "    \n",
    "    # Aggressive cleanup\n",
    "    try:\n",
    "        print(\"üßπ Aggressive cleanup of Chrome processes and temp files...\")\n",
    "        os.system(\"pkill -9 -f chrome 2>/dev/null || true\")\n",
    "        os.system(\"pkill -9 -f chromium 2>/dev/null || true\")\n",
    "        os.system(\"rm -rf /tmp/.org.chromium.* 2>/dev/null || true\")\n",
    "        os.system(\"rm -rf /tmp/chrome_* 2>/dev/null || true\")\n",
    "        time.sleep(3)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Try headless mode first - this avoids most user data conflicts\n",
    "    chrome_options = ChromeOptions()\n",
    "    \n",
    "    # HEADLESS mode - avoids user interface conflicts entirely\n",
    "    chrome_options.add_argument('--headless=new')  # Use new headless mode\n",
    "    \n",
    "    # Essential arguments for headless operation\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    chrome_options.add_argument('--disable-extensions')\n",
    "    chrome_options.add_argument('--disable-plugins')\n",
    "    chrome_options.add_argument('--disable-images')\n",
    "    chrome_options.add_argument('--no-first-run')\n",
    "    chrome_options.add_argument('--disable-default-apps')\n",
    "    chrome_options.add_argument('--disable-sync')\n",
    "    chrome_options.add_argument('--disable-background-networking')\n",
    "    chrome_options.add_argument('--disable-background-timer-throttling')\n",
    "    chrome_options.add_argument('--disable-renderer-backgrounding')\n",
    "    chrome_options.add_argument('--disable-features=TranslateUI,VizDisplayCompositor')\n",
    "    \n",
    "    # Set a reasonable window size for headless mode\n",
    "    chrome_options.add_argument('--window-size=1920,1080')\n",
    "    \n",
    "    # Use random port\n",
    "    debug_port = random.randint(20000, 60000)\n",
    "    chrome_options.add_argument(f'--remote-debugging-port={debug_port}')\n",
    "    \n",
    "    # Disable automation detection\n",
    "    chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "    \n",
    "    # Add user agent to avoid detection\n",
    "    chrome_options.add_argument('--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "    \n",
    "    # Try creating the driver\n",
    "    driver = None\n",
    "    max_attempts = 3\n",
    "    \n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            print(f\"üöÄ Attempting to start Chrome in HEADLESS mode (attempt {attempt + 1}/{max_attempts})...\")\n",
    "            print(f\"   Debug port: {debug_port}\")\n",
    "            print(\"   Mode: Headless (should avoid user data conflicts)\")\n",
    "            \n",
    "            driver = webdriver.Chrome(\n",
    "                service=ChromeService(ChromeDriverManager().install()), \n",
    "                options=chrome_options\n",
    "            )\n",
    "            \n",
    "            # Test the driver\n",
    "            driver.get(\"about:blank\")\n",
    "            print(\"‚úÖ Chrome started successfully in headless mode!\")\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            \n",
    "            if driver:\n",
    "                try:\n",
    "                    driver.quit()\n",
    "                except:\n",
    "                    pass\n",
    "                driver = None\n",
    "            \n",
    "            if attempt < max_attempts - 1:\n",
    "                # Aggressive cleanup before retry\n",
    "                try:\n",
    "                    os.system(\"pkill -9 -f chrome 2>/dev/null || true\")\n",
    "                    time.sleep(2)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # New port and fresh options\n",
    "                debug_port = random.randint(20000, 60000)\n",
    "                chrome_options = ChromeOptions()\n",
    "                chrome_options.add_argument('--headless=new')\n",
    "                chrome_options.add_argument('--no-sandbox')\n",
    "                chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "                chrome_options.add_argument('--disable-gpu')\n",
    "                chrome_options.add_argument('--window-size=1920,1080')\n",
    "                chrome_options.add_argument(f'--remote-debugging-port={debug_port}')\n",
    "                chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "                chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "                chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "                chrome_options.add_argument('--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "                \n",
    "                print(f\"   Retrying with new debug port: {debug_port}\")\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                print(\"‚ùå All headless attempts failed\")\n",
    "                return 0\n",
    "    \n",
    "    if not driver:\n",
    "        print(\"‚ùå Failed to start Chrome driver\")\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        # Perform automated login\n",
    "        print(\"üîë Starting automated login process...\")\n",
    "        if not automated_login(driver, username, password):\n",
    "            print(\"‚ùå Login failed. Cannot proceed with scraping.\")\n",
    "            return 0\n",
    "        \n",
    "        print(\"‚úÖ Login successful! Proceeding with data scraping...\")\n",
    "        \n",
    "        # Define strategies to scrape\n",
    "        strategies = [\n",
    "            {\n",
    "                \"url\": \"https://optionrecom.com/bear-call-spread-strategy/\",\n",
    "                \"type\": \"Bear Call\"\n",
    "            },\n",
    "            {\n",
    "                \"url\": \"https://optionrecom.com/bull-put-spread-strategy/\",\n",
    "                \"type\": \"Bull Put\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Process each strategy page\n",
    "        total_records = 0\n",
    "        \n",
    "        for strategy in strategies:\n",
    "            records = process_strategy_page(driver, strategy[\"url\"], strategy[\"type\"], conn, cursor)\n",
    "            total_records += records\n",
    "            time.sleep(3)  # Delay between strategy pages\n",
    "        \n",
    "        print(f\"\\nüéØ Total records saved to PostgreSQL database: {total_records}\")\n",
    "        \n",
    "        # Query to show what was saved to PostgreSQL (FIXED: proper timestamp casting)\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT strategy_type, tab_name, COUNT(*) as count \n",
    "            FROM option_strategies \n",
    "            WHERE scrape_date::timestamp >= NOW() - INTERVAL '1 hour' \n",
    "            GROUP BY strategy_type, tab_name\n",
    "        \"\"\")\n",
    "        results = cursor.fetchall()\n",
    "        \n",
    "        print(\"\\nüìä Records by strategy and tab (last hour) in PostgreSQL:\")\n",
    "        for strategy, tab, count in results:\n",
    "            print(f\"  {strategy} - {tab}: {count} records\")\n",
    "        \n",
    "        # Show total record count in PostgreSQL\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM option_strategies\")\n",
    "        total_count = cursor.fetchone()[0]\n",
    "        print(f\"\\nüìà Total records in PostgreSQL database: {total_count}\")\n",
    "        \n",
    "        # Note: headless mode can't keep browser open for manual use\n",
    "        if keep_browser_open:\n",
    "            print(\"\\nüí° Note: Headless mode doesn't support keeping browser open.\")\n",
    "            print(\"   The scraping completed successfully in the background.\")\n",
    "        \n",
    "        return total_records\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå An error occurred: {str(e)}\")\n",
    "        import traceback\n",
    "        print(f\"Full traceback: {traceback.format_exc()}\")\n",
    "        return 0\n",
    "    finally:\n",
    "        if driver:\n",
    "            try:\n",
    "                driver.quit()\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Fixed Scraper\n",
    "\n",
    "Execute the cell below to run the fixed automated scraper with improved table detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting IMPROVED PostgreSQL scraper with robust Chrome handling\n",
      "üìù Features: Process cleanup, incognito mode, retry logic, random ports\n",
      "üîê Using credentials from config/credentials.json\n",
      "‚úÖ Web scraping credentials loaded from config/credentials.json\n",
      "‚úÖ Connected to PostgreSQL: 35.204.11.121\n",
      "üßπ Aggressive cleanup of Chrome processes and temp files...\n",
      "üöÄ Attempting to start Chrome in HEADLESS mode (attempt 1/3)...\n",
      "   Debug port: 21216\n",
      "   Mode: Headless (should avoid user data conflicts)\n",
      "‚úÖ Chrome started successfully in headless mode!\n",
      "üîë Starting automated login process...\n",
      "Login attempt 1 of 3...\n",
      "Username entered successfully\n",
      "Password entered successfully\n",
      "Login button clicked (normal click)\n",
      "Login successful!\n",
      "‚úÖ Login successful! Proceeding with data scraping...\n",
      "\n",
      "===== Processing Bear Call Strategy Page =====\n",
      "Page date: August\n",
      "Found 8 tabs using selector: //div[contains(@class, 'ep_tabs_header')]//a[contains(@class, 'ep_label_main')]\n",
      "Tab 1: 'Mild Risk 95-97% accuracy > shorter expiry' -> https://optionrecom.com/bear-call-spread-strategy/#ep_tab_wrapper__5ad60c1c-00e1-4113-ac82-05579662dfae\n",
      "Tab 2: 'Minimal Risk 97-99% accuracy > shorter expiry' -> https://optionrecom.com/bear-call-spread-strategy/#ep_tab_wrapper__bfa5a7f9-7677-4d44-b8c4-1bb21745acf2\n",
      "Tab 3: 'Mild Risk 95-97% accuracy > longer expiry' -> https://optionrecom.com/bear-call-spread-strategy/#ep_tab_wrapper__b3b3cd22-578a-4067-9fd4-04bb0802559c\n",
      "Tab 4: 'Minimal Risk 97-99% accuracy > longer expiry' -> https://optionrecom.com/bear-call-spread-strategy/#ep_tab_wrapper__bd04a25d-76c0-496c-be19-2450473807e1\n",
      "Tab 5: 'Mild Risk 95-97% accuracy > shorter expiry' -> https://optionrecom.com/bear-call-spread-strategy/#ep_tab_wrapper__537d4708-a8a4-4082-b363-44eb51006779\n",
      "Tab 6: 'Minimal Risk 97-99% accuracy > shorter expiry' -> https://optionrecom.com/bear-call-spread-strategy/#ep_tab_wrapper__2e0bc482-844a-41ef-9825-dc73e8de793a\n",
      "Tab 7: 'Mild Risk 95-97% accuracy > longer expiry' -> https://optionrecom.com/bear-call-spread-strategy/#ep_tab_wrapper__d4c9dbd2-7c72-4277-9a11-521d2549775e\n",
      "Tab 8: 'Minimal Risk 97-99% accuracy > longer expiry' -> https://optionrecom.com/bear-call-spread-strategy/#ep_tab_wrapper__ceb91ce6-a0f0-4c40-9963-c4a51e4d7689\n",
      "\n",
      "Processing Tab 1: 'Mild Risk 95-97% accuracy > shorter expiry'\n",
      "Found tab content for ID: ep_tab_wrapper__5ad60c1c-00e1-4113-ac82-05579662dfae\n",
      "Table type: Active trades\n",
      "Table headers: ['ID', 'Ticker', 'Trigger Price', 'Strike Price', 'Estimated Premium']\n",
      "Column mapping: {'ID': 0, 'Ticker': 1, 'Trigger Price': 2, 'Strike Price': 3, 'Estimated Premium': 4}\n",
      "Found 1 data rows\n",
      "Row 1: 5 cells\n",
      "Skipping row 1 - invalid ticker: 'None'\n",
      "Successfully saved 0 records from tab #1\n",
      "\n",
      "Processing Tab 2: 'Minimal Risk 97-99% accuracy > shorter expiry'\n",
      "Found tab content for ID: ep_tab_wrapper__bfa5a7f9-7677-4d44-b8c4-1bb21745acf2\n",
      "Table type: Active trades\n",
      "Table headers: ['ID', 'Ticker', 'Trigger Price', 'Strike Price', 'Estimated Premium']\n",
      "Column mapping: {'ID': 0, 'Ticker': 1, 'Trigger Price': 2, 'Strike Price': 3, 'Estimated Premium': 4}\n",
      "Found 1 data rows\n",
      "Row 1: 5 cells\n",
      "Skipping row 1 - invalid ticker: 'None'\n",
      "Successfully saved 0 records from tab #2\n",
      "\n",
      "Processing Tab 3: 'Mild Risk 95-97% accuracy > longer expiry'\n",
      "Found tab content for ID: ep_tab_wrapper__b3b3cd22-578a-4067-9fd4-04bb0802559c\n",
      "Table type: Active trades\n",
      "Table headers: ['ID', 'Ticker', 'Trigger Price', 'Strike Price', 'Estimated Premium']\n",
      "Column mapping: {'ID': 0, 'Ticker': 1, 'Trigger Price': 2, 'Strike Price': 3, 'Estimated Premium': 4}\n",
      "Found 1 data rows\n",
      "Row 1: 5 cells\n",
      "Processing: IWM | Trigger: 239.17 | Strike: sell 250.0 - buy 260.0 | Premium: 102 | Trade ID: impressive-adamant-k...\n",
      "Successfully saved 1 records from tab #3\n",
      "\n",
      "Processing Tab 4: 'Minimal Risk 97-99% accuracy > longer expiry'\n",
      "Found tab content for ID: ep_tab_wrapper__bd04a25d-76c0-496c-be19-2450473807e1\n",
      "Table type: Active trades\n",
      "Table headers: ['ID', 'Ticker', 'Trigger Price', 'Strike Price', 'Estimated Premium']\n",
      "Column mapping: {'ID': 0, 'Ticker': 1, 'Trigger Price': 2, 'Strike Price': 3, 'Estimated Premium': 4}\n",
      "Found 1 data rows\n",
      "Row 1: 5 cells\n",
      "Skipping row 1 - invalid ticker: 'None'\n",
      "Successfully saved 0 records from tab #4\n",
      "\n",
      "===== Processing Bull Put Strategy Page =====\n",
      "Page date: August\n",
      "Found 8 tabs using selector: //div[contains(@class, 'ep_tabs_header')]//a[contains(@class, 'ep_label_main')]\n",
      "Tab 1: 'Mild Risk 95-97% accuracy > shorter expiry' -> https://optionrecom.com/bull-put-spread-strategy/#ep_tab_wrapper__9af1741d-5bc6-4146-b806-020f93802cff\n",
      "Tab 2: 'Minimal Risk 97-99% accuracy > shorter expiry' -> https://optionrecom.com/bull-put-spread-strategy/#ep_tab_wrapper__a446663b-f39d-4edd-a1bb-cc14f42bc0da\n",
      "Tab 3: 'Mild Risk 95-97% accuracy > longer expiry' -> https://optionrecom.com/bull-put-spread-strategy/#ep_tab_wrapper__ae6adbca-854f-4d42-9da0-4fb05b14f5d7\n",
      "Tab 4: 'Minimal Risk 97-99% accuracy > longer expiry' -> https://optionrecom.com/bull-put-spread-strategy/#ep_tab_wrapper__83d4f71f-930c-4482-8db9-4aa0ddb777a1\n",
      "Tab 5: 'Mild Risk 95-97% accuracy > shorter expiry' -> https://optionrecom.com/bull-put-spread-strategy/#ep_tab_wrapper__7c9b6de8-aa8e-433c-8dcd-1c39d3109a0d\n",
      "Tab 6: 'Minimal Risk 97-99% accuracy > shorter expiry' -> https://optionrecom.com/bull-put-spread-strategy/#ep_tab_wrapper__d517e96d-b9b3-496c-bbab-4d9d386102e3\n",
      "Tab 7: 'Mild Risk 95-97% accuracy > longer expiry' -> https://optionrecom.com/bull-put-spread-strategy/#ep_tab_wrapper__23fce7d6-88b7-4b58-8a89-143dac9322e5\n",
      "Tab 8: 'Minimal Risk 97-99% accuracy > longer expiry' -> https://optionrecom.com/bull-put-spread-strategy/#ep_tab_wrapper__0a31bfdf-74ce-44b3-817b-458e9b9b6d67\n",
      "\n",
      "Processing Tab 1: 'Mild Risk 95-97% accuracy > shorter expiry'\n",
      "Found tab content for ID: ep_tab_wrapper__9af1741d-5bc6-4146-b806-020f93802cff\n",
      "Table type: Active trades\n",
      "Table headers: ['ID', 'Ticker', 'Trigger Price', 'Strike Price', 'Estimated Premium']\n",
      "Column mapping: {'ID': 0, 'Ticker': 1, 'Trigger Price': 2, 'Strike Price': 3, 'Estimated Premium': 4}\n",
      "Found 2 data rows\n",
      "Row 1: 5 cells\n",
      "Processing: META | Trigger: 726.22 | Strike: sell 665.0 - buy 655.0 | Premium: 75 | Trade ID: nifty-liberal-worm...\n",
      "Row 2: 5 cells\n",
      "Processing: V | Trigger: 335.58 | Strike: sell 320.0 - buy 310.0 | Premium: 80 | Trade ID: judicious-cyan-carac...\n",
      "Successfully saved 2 records from tab #1\n",
      "\n",
      "Processing Tab 2: 'Minimal Risk 97-99% accuracy > shorter expiry'\n",
      "Found tab content for ID: ep_tab_wrapper__a446663b-f39d-4edd-a1bb-cc14f42bc0da\n",
      "Table type: Active trades\n",
      "Table headers: ['ID', 'Ticker', 'Trigger Price', 'Strike Price', 'Estimated Premium']\n",
      "Column mapping: {'ID': 0, 'Ticker': 1, 'Trigger Price': 2, 'Strike Price': 3, 'Estimated Premium': 4}\n",
      "Found 1 data rows\n",
      "Row 1: 5 cells\n",
      "Skipping row 1 - invalid ticker: 'None'\n",
      "Successfully saved 0 records from tab #2\n",
      "\n",
      "Processing Tab 3: 'Mild Risk 95-97% accuracy > longer expiry'\n",
      "Found tab content for ID: ep_tab_wrapper__ae6adbca-854f-4d42-9da0-4fb05b14f5d7\n",
      "Table type: Active trades\n",
      "Table headers: ['ID', 'Ticker', 'Trigger Price', 'Strike Price', 'Estimated Premium']\n",
      "Column mapping: {'ID': 0, 'Ticker': 1, 'Trigger Price': 2, 'Strike Price': 3, 'Estimated Premium': 4}\n",
      "Found 1 data rows\n",
      "Row 1: 5 cells\n",
      "Processing: WMT | Trigger: 95.66 | Strike: sell 93.0 - buy 85.0 | Premium: 80 | Trade ID: new-capable-shrimp...\n",
      "Successfully saved 1 records from tab #3\n",
      "\n",
      "Processing Tab 4: 'Minimal Risk 97-99% accuracy > longer expiry'\n",
      "Found tab content for ID: ep_tab_wrapper__83d4f71f-930c-4482-8db9-4aa0ddb777a1\n",
      "Table type: Active trades\n",
      "Table headers: ['ID', 'Ticker', 'Trigger Price', 'Strike Price', 'Estimated Premium']\n",
      "Column mapping: {'ID': 0, 'Ticker': 1, 'Trigger Price': 2, 'Strike Price': 3, 'Estimated Premium': 4}\n",
      "Found 1 data rows\n",
      "Row 1: 5 cells\n",
      "Skipping row 1 - invalid ticker: 'None'\n",
      "Successfully saved 0 records from tab #4\n",
      "\n",
      "üéØ Total records saved to PostgreSQL database: 4\n",
      "\n",
      "üìä Records by strategy and tab (last hour) in PostgreSQL:\n",
      "  Bull Put - Mild Risk 95-97% accuracy > shorter expiry: 2 records\n",
      "  Bear Call - Mild Risk 95-97% accuracy > longer expiry: 1 records\n",
      "  Bull Put - Mild Risk 95-97% accuracy > longer expiry: 1 records\n",
      "\n",
      "üìà Total records in PostgreSQL database: 563\n",
      "\n",
      "üí° Note: Headless mode doesn't support keeping browser open.\n",
      "   The scraping completed successfully in the background.\n",
      "\n",
      "üéâ PostgreSQL scraper completed successfully!\n",
      "üìä Total records processed: 4\n",
      "üíæ Data written directly to PostgreSQL database at 35.204.11.121\n",
      "üîÑ Run your utility notebook to see the new data!\n"
     ]
    }
   ],
   "source": [
    "# Run the improved automated scraper with robust Chrome startup\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting IMPROVED PostgreSQL scraper with robust Chrome handling\")\n",
    "    print(\"üìù Features: Process cleanup, incognito mode, retry logic, random ports\")\n",
    "    print(\"üîê Using credentials from config/credentials.json\")\n",
    "    \n",
    "    result = scrape_option_strategies_automated(\n",
    "        keep_browser_open=True  # Set to False if you want the browser to close automatically\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüéâ PostgreSQL scraper completed successfully!\")\n",
    "    print(f\"üìä Total records processed: {result}\")\n",
    "    print(\"üíæ Data written directly to PostgreSQL database at 35.204.11.121\")\n",
    "    print(\"üîÑ Run your utility notebook to see the new data!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optcom3_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
